{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "3a82d887-fbbf-4226-80b3-7f6cd5ac27e8",
      "cell_type": "code",
      "source": "import re\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "d0da8b46-3483-4aa7-9edc-90ded8cbe83a",
      "cell_type": "code",
      "source": "class DataCleaner:\n    def __init__(self, df):\n        self.df = df\n        self.column_scope = ['comment_count', 'dislike_count', 'like_count', 'view_count', 'licensed_content',\n                             'duration_sec', 'video_category_label', 'video_title', 'video_description', 'published_at',\n                             'definition', 'duration']\n    def clean_desc(self, desc):\n        desc = desc.split('#BBC')[0]\n        desc = re.split(r' https://bbc\\.in\\w*', desc)[-1]\n        return desc\n        \n    def clean_licensed(self, lin):\n        # T/F instead of 1/nan\n        try:\n            int(lin)\n            return True\n        except ValueError:\n            if type(lin) is float:\n                return False\n\n    def parse_duration(self, duration_str):\n        duration_str = duration_str[2:]\n        total_seconds = 0\n        if 'H' in duration_str:\n            hours, duration_str = duration_str.split('H')\n            total_seconds += int(hours) * 3600\n        if 'M' in duration_str:\n            minutes, duration_str = duration_str.split('M')\n            total_seconds += int(minutes) * 60\n        if 'S' in duration_str:\n            seconds = duration_str.rstrip('S')\n            total_seconds += int(seconds)\n        return total_seconds\n        \n    def clean(self):\n        # Get only valid columns\n        self.df = self.df[self.column_scope]\n        # Clean video desc\n        self.df['video_description_cleaned'] = self.df['video_description'].apply(cleaner.clean_desc)\n        # Change licensced content format to bool\n        self.df['licensed_content'] = self.df['licensed_content'].apply(cleaner.clean_licensed)\n        # Change published at type to datetime\n        self.df['published_at'] = self.df['published_at'].apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ'))\n        # Parsing duration\n        self.df['duration'] = self.df['duration'].apply(self.parse_duration)\n        # DF description\n        \n        return self.df\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "a8985820-7de8-494c-9fab-35e3a1c7a609",
      "cell_type": "code",
      "source": "class BBC:\n    def __init__(self, df):\n        self.df = df\n        nltk.download('stopwords')\n        nltk.download('punkt')\n        self.stop_words = set(stopwords.words('english'))\n        self.stop_words = self.stop_words.union({'bbc', 'two', 'one', 'three',\n                              'part', 'series', 'episode', 'preview', 'show'})\n\n    def data_types(self):\n        for column in list(self.df.columns):\n            print(f'column name: {column},\\n column values types: {self.df[column].apply(type).unique()},\\n'\n                  f' representative value: {self.df[column].unique()[1]}\\n###################\\n')\n\n    def date_check(self):\n        newest_date, oldest_date = self.df['published_at'].max().year, self.df['published_at'].min().year\n        print(f'newest article (YYYY): {newest_date},'\n              f'oldest article (YYYY): {oldest_date}')\n        self.df['published_parsed'] = self.df['published_at'].dt.year\n\n    def top_categories(self):\n        top_occurrences = self.df['video_category_label'].value_counts().head(5)\n        colors = ['#004c6d', '#00587a', '#00668e', '#0074a2',  '#0083b6',\n                  '#0091c9', '#00a0dd', '#00aee1', '#00bcf4', '#00cbff']\n        plt.figure(figsize=(8, 8))\n        patches, texts, autotexts = plt.pie(top_occurrences, labels=top_occurrences.index, autopct='%1.1f%%', colors=colors)\n        for autotext in autotexts:\n            autotext.set_color('white')\n        plt.title('Top 5 Occurrences')\n        plt.show()\n\n    def clean_title(self):\n        def _clean_title(title):\n            title = title.translate(str.maketrans('', '', string.punctuation))\n            title = re.sub(r'\\d+', '', title)\n            tokens = word_tokenize(title)\n            tokens = [word.lower() for word in tokens if word.lower() not in self.stop_words]\n            return tokens\n        self.df['video_title_clean'] = self.df['video_title'].apply(_clean_title)\n\n    def top_title_keywords(self):\n        _df = self.df\n        grouped = _df.groupby('published_parsed')['video_title_clean'].sum()\n        keyword_counts = grouped.apply(Counter)\n        top_keywords_by_year = keyword_counts.apply(lambda x: [word for word, _ in x.most_common(5)])\n        print(top_keywords_by_year)\n        top_keywords_by_year = keyword_counts.apply(lambda x: [word for word, _ in x.most_common(2)])\n        keyword_counts = top_keywords_by_year.explode().groupby(level=0).value_counts().unstack(fill_value=0)\n        keyword_counts.plot(kind='bar', stacked=True, figsize=(10, 6))\n        plt.title('Top 2 Keywords in Titles Each Year')\n        plt.xlabel('Year')\n        plt.ylabel('Number of Occurrences')\n        plt.legend(title='Keywords')\n        plt.xticks(rotation=45)\n        plt.show()\n        del _df\n\n\n    def engagement_rate(self):\n        self.df['engagement_rate'] = ((self.df['like_count'] + self.df['comment_count'] + self.df['dislike_count']) / self.df['view_count']) * 100\n        self.df['engagement_rate'] = self.df['engagement_rate'].round(1)\n\n    def title_len(self):\n        self.df['title_len'] = self.df['video_title_clean'].apply(lambda x: sum(len(word) for word in x))\n\n    def dichotomized_engagement(self):\n        median_rate = self.df['engagement_rate'].median()\n        self.df['dichotomized_engagement'] = (self.df['engagement_rate'] >= median_rate).astype(int)\n\n    def encode2numeric(self, *args):\n        for column in args:\n            self.df[column], _ = pd.factorize(self.df[column])\n\n    def visualise_correlations(self):\n        correlation_matrix = self.df[\n            ['definition', 'duration', 'dichotomized_engagement', 'published_parsed', 'engagement_rate', 'title_len',\n             'video_category_label']].corr()\n        \n        plt.figure(figsize=(10, 8))\n        plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')\n        plt.colorbar()\n        plt.title('Correlation Matrix of BBC YouTube Videos Metadata')\n        plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)\n        plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n        plt.show()\n\n        plt.figure(figsize=(12, 8))\n        for i, col in enumerate(['duration', 'engagement_rate', 'title_len']):\n            plt.subplot(2, 2, i + 1)\n            plt.scatter(self.df[col], self.df['dichotomized_engagement'], alpha=0.5)\n            plt.xlabel(col)\n            plt.ylabel('Dichotomized Score')\n            plt.title(f'Scatter plot: Dichotomized Score vs {col}')\n        plt.tight_layout()\n        plt.show()\n\n    def run(self):\n        self.data_types()\n        self.date_check()\n        self.top_categories()\n        \n        self.clean_title()\n        self.top_title_keywords()\n        self.engagement_rate()\n        self.title_len()\n        self.dichotomized_engagement()\n        self.encode2numeric('video_category_label', 'definition')\n        self.visualise_correlations()\n        self.df.drop(columns=['published_at', 'licensed_content', 'duration'], inplace=True)\n        # IMO 'licensed_content' and 'duration' may bring valuable correlations while analyzing the dataset\n        # ... but im dropping them since thats the task (:\n        ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "9c89f827-8b14-40f0-8142-e69259761506",
      "cell_type": "code",
      "source": "df = pd.read_csv('bbc.csv')\n\nprint('###########\\n', df.head(), '\\n')  # Display the first few rows of the DataFrame\nprint('###########\\n', df.info(), '\\n')   # Display information about the DataFrame\nprint('###########\\n', df.describe(), '\\n')\ncleaner = DataCleaner(df)\ncleaned_df = cleaner.clean()\nbbc = BBC(cleaned_df)\nbbc.run()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "###########\n    position                channel_id channel_title     video_id  \\\n0         1  UCCj956IF62FbT7Gouszaj9w           BBC  8qH0pGdjB_U   \n1         2  UCCj956IF62FbT7Gouszaj9w           BBC  lqeS-rOoBSw   \n2         3  UCCj956IF62FbT7Gouszaj9w           BBC  JMfkBavl1ks   \n3         4  UCCj956IF62FbT7Gouszaj9w           BBC  T_6RRmkLOSs   \n4         5  UCCj956IF62FbT7Gouszaj9w           BBC  3-mayD_9Yg8   \n\n           published_at                                        video_title  \\\n0  2020-08-13T15:00:02Z  Colin Robinson's Origins of the Species - What...   \n1  2020-08-13T14:30:04Z  Maisie Smith and Zack Morris on EastEnders' la...   \n2  2020-08-13T05:50:21Z  A-level results to arrive in year with no exam...   \n3  2020-08-12T13:00:13Z  8 signs you're in survival mode and how to sta...   \n4  2020-08-12T11:00:02Z  The secret Heathrow lounge that costs Â£2700 ju...   \n\n                                   video_description  video_category_id  \\\n0  Subscribe and ðŸ”” to OFFICIAL BBC YouTube ðŸ‘‰ http...                 24   \n1  Subscribe and ðŸ”” to OFFICIAL BBC YouTube ðŸ‘‰ http...                 24   \n2  Subscribe and ðŸ”” to OFFICIAL BBC YouTube ðŸ‘‰ http...                 27   \n3  Subscribe and ðŸ”” to OFFICIAL BBC YouTube ðŸ‘‰ http...                 27   \n4  Subscribe and ðŸ”” to OFFICIAL BBC YouTube ðŸ‘‰ http...                 24   \n\n  video_category_label  duration  duration_sec dimension definition  caption  \\\n0        Entertainment   PT5M23S           323        2d         hd    False   \n1        Entertainment   PT3M15S           195        2d         hd    False   \n2            Education  PT14M48S           888        2d         hd    False   \n3            Education   PT3M50S           230        2d         hd    False   \n4        Entertainment   PT1M52S           112        2d         hd    False   \n\n   licensed_content  view_count  like_count  dislike_count  favorite_count  \\\n0               1.0         738        76.0            7.0               0   \n1               1.0         512        55.0            9.0               0   \n2               NaN       19888       326.0           50.0               0   \n3               1.0       14515       324.0          532.0               0   \n4               1.0       15644       331.0           14.0               0   \n\n   comment_count  \n0            4.0  \n1           13.0  \n2          128.0  \n3          282.0  \n4           22.0   \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 12456 entries, 0 to 12455\nData columns (total 20 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   position              12456 non-null  int64  \n 1   channel_id            12456 non-null  object \n 2   channel_title         12456 non-null  object \n 3   video_id              12456 non-null  object \n 4   published_at          12456 non-null  object \n 5   video_title           12456 non-null  object \n 6   video_description     12456 non-null  object \n 7   video_category_id     12456 non-null  int64  \n 8   video_category_label  12456 non-null  object \n 9   duration              12456 non-null  object \n 10  duration_sec          12456 non-null  int64  \n 11  dimension             12456 non-null  object \n 12  definition            12456 non-null  object \n 13  caption               12456 non-null  bool   \n 14  licensed_content      11878 non-null  float64\n 15  view_count            12456 non-null  int64  \n 16  like_count            12454 non-null  float64\n 17  dislike_count         12454 non-null  float64\n 18  favorite_count        12456 non-null  int64  \n 19  comment_count         12361 non-null  float64\ndtypes: bool(1), float64(4), int64(5), object(10)\nmemory usage: 1.3+ MB\n###########\n None \n\n###########\n            position  video_category_id  duration_sec  licensed_content  \\\ncount  12456.000000       12456.000000  12456.000000           11878.0   \nmean    6228.500000          22.893465    200.663937               1.0   \nstd     3595.881811           3.708634    309.089602               0.0   \nmin        1.000000           1.000000     11.000000               1.0   \n25%     3114.750000          24.000000     93.000000               1.0   \n50%     6228.500000          24.000000    132.000000               1.0   \n75%     9342.250000          24.000000    206.000000               1.0   \nmax    12456.000000          29.000000   3536.000000               1.0   \n\n         view_count    like_count  dislike_count  favorite_count  \\\ncount  1.245600e+04  1.245400e+04   12454.000000         12456.0   \nmean   5.542936e+05  3.824802e+03     149.052353             0.0   \nstd    2.334346e+06  2.186700e+04     788.518681             0.0   \nmin    5.120000e+02  2.000000e+00       0.000000             0.0   \n25%    4.485100e+04  1.850000e+02       8.000000             0.0   \n50%    1.036485e+05  5.185000e+02      21.000000             0.0   \n75%    3.215382e+05  1.874000e+03      68.000000             0.0   \nmax    1.047765e+08  1.092624e+06   33349.000000             0.0   \n\n       comment_count  \ncount   12361.000000  \nmean      308.681256  \nstd      1461.156794  \nmin         0.000000  \n25%        23.000000  \n50%        56.000000  \n75%       172.000000  \nmax     82261.000000   \n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "<ipython-input-2-cf782b798fe9>:39: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self.df['video_description_cleaned'] = self.df['video_description'].apply(cleaner.clean_desc)\n<ipython-input-2-cf782b798fe9>:41: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self.df['licensed_content'] = self.df['licensed_content'].apply(cleaner.clean_licensed)\n<ipython-input-2-cf782b798fe9>:43: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self.df['published_at'] = self.df['published_at'].apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ'))\n<ipython-input-2-cf782b798fe9>:45: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self.df['duration'] = self.df['duration'].apply(self.parse_duration)\n[nltk_data] Error loading stopwords: <urlopen error [Errno 23] Host is\n[nltk_data]     unreachable>\n[nltk_data] Error loading punkt: <urlopen error [Errno 23] Host is\n[nltk_data]     unreachable>\n",
          "output_type": "stream"
        },
        {
          "ename": "<class 'LookupError'>",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/pyodide/nltk_data'\n    - '/nltk_data'\n    - '/share/nltk_data'\n    - '/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m/lib/python3.11/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/pyodide/nltk_data'\n    - '/nltk_data'\n    - '/share/nltk_data'\n    - '/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m cleaner \u001b[38;5;241m=\u001b[39m DataCleaner(df)\n\u001b[1;32m      7\u001b[0m cleaned_df \u001b[38;5;241m=\u001b[39m cleaner\u001b[38;5;241m.\u001b[39mclean()\n\u001b[0;32m----> 8\u001b[0m bbc \u001b[38;5;241m=\u001b[39m \u001b[43mBBC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m bbc\u001b[38;5;241m.\u001b[39mrun()\n",
            "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mBBC.__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words\u001b[38;5;241m.\u001b[39munion({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthree\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpart\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreview\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshow\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/pyodide/nltk_data'\n    - '/nltk_data'\n    - '/share/nltk_data'\n    - '/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 6
    },
    {
      "id": "3a6e68ef-ba77-4ac7-a04a-67bcc3a1480a",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ff49c66f-84de-4f55-aad8-f4f451fca23e",
      "cell_type": "code",
      "source": "# The correlation graphs show that the most egagement is coming from short videos, but the really long ones also grab attentions. worst\n# While title len and category doesnt seem to play a role in a bigger time scope, and we can safely say that people dont care about definition. \n# Most engagement falls within close range from video to video with few excpetions which egagement is \"breaking the celling\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "id": "a2089912-0119-49ae-8e14-a164bbb4c9f0",
      "cell_type": "code",
      "source": "# Better correlation could be derived from analyzing description",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "108edfa0-5609-4190-9da2-4419f2e2d07c",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}